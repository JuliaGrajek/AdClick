{
 "cells": [
  {
   "cell_type": "code",
   "execution_count":  1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures, Embedding, LSTM, concatenate, Conv1D, Flatten, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train =  r'C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\D80M\\D80M.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to save and load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_enc(enc, name ):\n",
    "    with open('AdClick_obj/'+ name + '.pkl', 'wb+') as f:     \n",
    "        pickle.dump({'config': enc.get_config(),\n",
    "             'weights': enc.get_weights()}\n",
    "            , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enc( name ):\n",
    "    with open('AdClick_obj/'+ name + '.pkl', 'rb') as f:     \n",
    "        load_enc = pickle.load(f)\n",
    "    new_vec = preprocessing.TextVectorization.from_config(load_enc['config'])\n",
    "    new_vec.set_weights(load_enc['weights'])\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name ):\n",
    "    with open('AdClick_obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of boundaries that will be used to bucketize data from the DisplayURL column. I have determined them in a different script with the code: seq=np.linspace(min(X_train[\"DisplayURL\"]), max(X_train[\"DisplayURL\"]), 12000), where X_train was the train set turned into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = load_obj('DisplayURLbucketboundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into tensorflow dataset in batches using make_csv_dataset, add numerical column pos_per_depth and divide the dataset into train and holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col(X, y):\n",
    "    pos_per_depth = (X['Depth'] - X['Position']) / X['Depth']\n",
    "    X.update({'pos_per_depth': pos_per_depth})\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_path, batch_size, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=batch_size,\n",
    "        label_name='Click',\n",
    "        num_epochs=1,\n",
    "        field_delim='\\t',\n",
    "        shuffle=False,\n",
    "        ignore_errors=True,\n",
    "        **kwargs)\n",
    "    new_ds = dataset.map(add_col)\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use only 4 epochs because tensorflow datasets gave me some unexpected problems, so ultimately once the code was working I didn't have the time to learn the neural net for more epochs. If I had more time I would add way more epochs (ca 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got into make_csv_dataset_v2\n",
      "None\n",
      "Click\n",
      "['Click', 'DisplayURL', 'AdId', 'AdvertiserId', 'Depth', 'Position', 'Gender', 'Age', 'AdKeyword_tokens', 'AdTitle_tokens', 'AdDescription_tokens', 'Query_tokens']\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 4\n",
    "DATASET_SIZE = 80000000\n",
    "BATCH_SIZE = 256*4\n",
    "train_size = int(0.98 * DATASET_SIZE / BATCH_SIZE)\n",
    "SELECT_COLUMNS = ['Gender', 'Position', 'Depth', 'Age', 'AdvertiserId', 'AdId', 'DisplayURL', 'AdKeyword_tokens', 'AdDescription_tokens',\n",
    "                  'AdTitle_tokens', 'Query_tokens', 'Click']\n",
    "\n",
    "full_dataset = get_dataset(filepath_train, batch_size=BATCH_SIZE, select_columns=SELECT_COLUMNS)\n",
    "#full_dataset = full_dataset.shuffle(reshuffle_each_iteration=False, buffer_size=DATASET_SIZE // BATCH_SIZE)\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "val_dataset = full_dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the multiple input model work with the data read using csv_make_dataset I had to first map each column to a dataset containing only this column and then zip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text columns\n",
    "adkeyword_train_ds = train_dataset.map(lambda x, y: x['AdKeyword_tokens'])\n",
    "adkeyword_val_ds = val_dataset.map(lambda x, y: x['AdKeyword_tokens'])\n",
    "addescription_train_ds = train_dataset.map(lambda x, y: x['AdDescription_tokens'])\n",
    "addescription_val_ds = val_dataset.map(lambda x, y: x['AdDescription_tokens'])\n",
    "adtitle_train_ds = train_dataset.map(lambda x, y: x['AdTitle_tokens'])\n",
    "adtitle_val_ds = val_dataset.map(lambda x, y: x['AdTitle_tokens'])\n",
    "query_train_ds = train_dataset.map(lambda x, y: x['Query_tokens'])\n",
    "query_val_ds = val_dataset.map(lambda x, y: x['Query_tokens'])\n",
    "\n",
    "#Categorical Variable columns\n",
    "gender_train_ds = train_dataset.map(lambda x, y: x['Gender'])\n",
    "gender_val_ds = val_dataset.map(lambda x, y: x['Gender'])\n",
    "age_train_ds = train_dataset.map(lambda x, y: x['Age'])\n",
    "age_val_ds = val_dataset.map(lambda x, y: x['Age'])\n",
    "depth_train_ds = train_dataset.map(lambda x, y: x['Depth'])\n",
    "depth_val_ds = val_dataset.map(lambda x, y: x['Depth'], )\n",
    "position_train_ds = train_dataset.map(lambda x, y: x['Position'])\n",
    "position_val_ds = val_dataset.map(lambda x, y: x['Position'])\n",
    "AdvertiserId_train_ds = train_dataset.map(lambda x, y: x['AdvertiserId'])\n",
    "AdvertiserId_val_ds = val_dataset.map(lambda x, y: x['AdvertiserId'])\n",
    "AdId_train_ds = train_dataset.map(lambda x, y: x['AdId'])\n",
    "AdId_val_ds = val_dataset.map(lambda x, y: x['AdId'])\n",
    "url_train_ds = train_dataset.map(lambda x, y: x['DisplayURL'])\n",
    "url_val_ds = val_dataset.map(lambda x, y: x['DisplayURL'])\n",
    "\n",
    "#Numerical Columns\n",
    "pospdepth_train_ds = train_dataset.map(lambda x, y: x['pos_per_depth'])\n",
    "pospdepth_val_ds = val_dataset.map(lambda x, y: x['pos_per_depth'])\n",
    "\n",
    "#Label column\n",
    "y_train_ds = train_dataset.map(lambda x, y: y)\n",
    "y_val_ds = val_dataset.map(lambda x, y: y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode categorical variables that only take a few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Gender': [0, 1, 2],\n",
    "    'Position': [1, 2, 3],\n",
    "    'Age': [1, 2, 3, 4, 5, 6],\n",
    "    'Depth': [1,2 ,3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "    categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use hashing for categorical variables that take a lot of values to decrease the number of columns + account for values that are present in the test set but not in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_CATEGORIES = {'AdId':250000, 'AdvertiserId':8000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, hash_bucket_size in HASH_CATEGORIES.items():\n",
    "    cat_hashed = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      feature, hash_bucket_size=hash_bucket_size, dtype=tf.int64)\n",
    "    categorical_columns.append(tf.feature_column.indicator_column(cat_hashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had trouble hashing DisplayURL because categorical_column_with_hash_bucket only accepts integer or string columns, so I decided to bucketize it instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets =seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = tf.feature_column.numeric_column('DisplayURL')\n",
    "cat_bucketed=tf.feature_column.bucketized_column(\n",
    "    url, buckets)\n",
    "categorical_columns.append(tf.feature_column.indicator_column(cat_bucketed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I create the preprocessing layer for the columns containing tokens. I tokenize them using TextVectorization and then embed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split(input):\n",
    "    return tf.strings.split(input, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked the maximum number of tokens per record in a different script\n",
    "word_count_keyword= 15\n",
    "word_count_description = 50\n",
    "word_count_title = 30\n",
    "word_count_query = 128\n",
    "\n",
    "# Zmniejszyc do 1000\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ds(text_ds, word_count):\n",
    "    encoder = preprocessing.TextVectorization(split=my_split, output_mode=\"int\", max_tokens=VOCAB_SIZE, output_sequence_length=word_count)\n",
    "    encoder.adapt(text_ds)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_keyword = vectorize_ds(adkeyword_train_ds, word_count_keyword)\n",
    "encoder_description = vectorize_ds(addescription_train_ds, word_count_description)\n",
    "encoder_title= vectorize_ds(adtitle_train_ds, word_count_title)\n",
    "encoder_query = vectorize_ds(query_train_ds, word_count_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_preprocessing_layers(name, encoder):\n",
    "    text_input = Input(shape=(None,), name=name, dtype=tf.string)\n",
    "    text_features = encoder(text_input)\n",
    "    text_features = Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True)(text_features)\n",
    "\n",
    "# #     Zmienic to na flatten bez convow\n",
    "#     text_features=Conv1D(64, 3, padding='same')(text_features)\n",
    "#     text_features=Conv1D(64, 3, padding='same')(text_features)\n",
    "#     text_features=Conv1D(64, 3, padding='same')(text_features)\n",
    "    text_features = Flatten()(text_features)\n",
    "    text_features = tf.keras.Model(inputs=text_input, outputs=text_features)\n",
    "#     filepath = 'AdClick_obj/'+ name\n",
    "#     text_features.save(filepath, save_format=\"tf\")\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I build a multiple input model. String variables go through the text preprocessing layers, while categorical variables go through the preprocessing layer (one hot encoding/ hashing). The layers are concatenated and then I just use Dense layers and a few Dropout layers to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    keywords_model = create_text_preprocessing_layers(\"Adkeyword\", encoder_keyword)\n",
    "    description_model = create_text_preprocessing_layers(\"Addescription\", encoder_description)\n",
    "    title_model = create_text_preprocessing_layers(\"Adtitle\", encoder_title)\n",
    "    query_model = create_text_preprocessing_layers(\"Query\", encoder_query)\n",
    "\n",
    "    \n",
    "    feature_layer_inputs = {}\n",
    "    for header in ['Gender', 'Position', 'Age', 'Depth', 'AdvertiserId', 'AdId', 'DisplayURL']:\n",
    "        feature_layer_inputs[header] = Input(shape=(1,), name=header, dtype=tf.int64)\n",
    "\n",
    "    cat_features = DenseFeatures(categorical_columns)(feature_layer_inputs)\n",
    "    cat_features = tf.keras.Model(inputs=feature_layer_inputs, outputs=cat_features)\n",
    "\n",
    "    combined = concatenate([keywords_model.output, description_model.output, title_model.output, query_model.output,\n",
    "                            cat_features.output])\n",
    "\n",
    "\n",
    "    x = Dense(128, activation ='relu')(combined)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "#     x = Dense(32, activation = 'relu')(combined)\n",
    "    x = Dense(10, activation = 'relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[keywords_model.input, description_model.input, title_model.input, query_model.input, feature_layer_inputs],\n",
    "        outputs=x)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['AUC'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Adkeyword (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Addescription (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Adtitle (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Query (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization (TextVectori (None, 15)           0           Adkeyword[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_1 (TextVecto (None, 50)           0           Addescription[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_2 (TextVecto (None, 30)           0           Adtitle[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_3 (TextVecto (None, 128)          0           Query[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 15, 64)       640000      text_vectorization[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 64)       640000      text_vectorization_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 30, 64)       640000      text_vectorization_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 128, 64)      640000      text_vectorization_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "AdId (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "AdvertiserId (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Depth (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "DisplayURL (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Position (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 960)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3200)         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1920)         0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8192)         0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (None, 270016)       0           AdId[0][0]                       \n",
      "                                                                 AdvertiserId[0][0]               \n",
      "                                                                 Age[0][0]                        \n",
      "                                                                 Depth[0][0]                      \n",
      "                                                                 DisplayURL[0][0]                 \n",
      "                                                                 Gender[0][0]                     \n",
      "                                                                 Position[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 284288)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          36388992    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           650         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            11          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,957,909\n",
      "Trainable params: 38,957,909\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The early stopping callback isn't important anymore because I set the number of epochs to 4\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_auc\",\n",
    "        min_delta=1e-5,\n",
    "        mode='max',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    r\"C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\saved_models\\weights2.{epoch:02d}-{val_auc:.2f}\",\n",
    "    monitor=\"val_auc\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode=\"max\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model and make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = tf.data.Dataset.zip((adkeyword_train_ds, addescription_train_ds, adtitle_train_ds, query_train_ds, gender_train_ds, position_train_ds, age_train_ds, depth_train_ds, AdvertiserId_train_ds, AdId_train_ds, url_train_ds))\n",
    "dataset = tf.data.Dataset.zip((data_in, y_train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = tf.data.Dataset.zip((adkeyword_val_ds, addescription_val_ds, adtitle_val_ds, query_val_ds, gender_val_ds, position_val_ds, age_val_ds, depth_val_ds, AdvertiserId_val_ds, AdId_val_ds, url_val_ds))\n",
    "dataset_val = tf.data.Dataset.zip((data_val, y_val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available\", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "76562/76562 [==============================] - 18133s 237ms/step - loss: 0.1757 - auc: 0.7030 - val_loss: 0.1771 - val_auc: 0.7219\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.72188, saving model to C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\saved_models\\weights2.01-0.72\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\saved_models\\weights2.01-0.72\\assets\n",
      "Epoch 2/4\n",
      "76562/76562 [==============================] - 17452s 228ms/step - loss: 0.1719 - auc: 0.7260 - val_loss: 0.1769 - val_auc: 0.7233\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.72188 to 0.72335, saving model to C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\saved_models\\weights2.02-0.72\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\saved_models\\weights2.02-0.72\\assets\n",
      "Epoch 3/4\n",
      "76562/76562 [==============================] - 18433s 241ms/step - loss: 0.1715 - auc: 0.7291 - val_loss: 0.1769 - val_auc: 0.7231\n",
      "\n",
      "Epoch 00003: val_auc did not improve from 0.72335\n",
      "Epoch 4/4\n",
      "76562/76562 [==============================] - 18130s 237ms/step - loss: 0.1712 - auc: 0.7312 - val_loss: 0.1771 - val_auc: 0.7228\n",
      "\n",
      "Epoch 00004: val_auc did not improve from 0.72335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x246d8406100>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data = dataset_val,\n",
    "          callbacks=[early_stopping_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_test =  r'C:\\Users\\piotr\\OneDrive\\Pulpit\\Julia\\D5M_test_x\\D5M_test_x.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think shuffling is turned off :) I had to add column_defaults here because tensorflow would recognize some token columns as int instead of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got into make_csv_dataset_v2\n",
      "None\n",
      "Click\n",
      "['Click', 'DisplayURL', 'AdId', 'AdvertiserId', 'Depth', 'Position', 'Gender', 'Age', 'AdKeyword_tokens', 'AdTitle_tokens', 'AdDescription_tokens', 'Query_tokens']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_dataset(filepath_test, batch_size=BATCH_SIZE, select_columns=SELECT_COLUMNS, column_defaults = [tf.int32, tf.float32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.string, tf.string, tf.string, tf.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (OrderedDict([(DisplayURL, (None,)), (AdId, (None,)), (AdvertiserId, (None,)), (Depth, (None,)), (Position, (None,)), (Gender, (None,)), (Age, (None,)), (AdKeyword_tokens, (None,)), (AdTitle_tokens, (None,)), (AdDescription_tokens, (None,)), (Query_tokens, (None,)), (pos_per_depth, (None,))]), (None,)), types: (OrderedDict([(DisplayURL, tf.float32), (AdId, tf.int32), (AdvertiserId, tf.int32), (Depth, tf.int32), (Position, tf.int32), (Gender, tf.int32), (Age, tf.int32), (AdKeyword_tokens, tf.string), (AdTitle_tokens, tf.string), (AdDescription_tokens, tf.string), (Query_tokens, tf.string), (pos_per_depth, tf.float64)]), tf.int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "adkeyword_test_ds = test_dataset.map(lambda x, y: x['AdKeyword_tokens'])\n",
    "addescription_test_ds = test_dataset.map(lambda x, y: x['AdDescription_tokens'])\n",
    "adtitle_test_ds = test_dataset.map(lambda x, y: x['AdTitle_tokens'])\n",
    "query_test_ds = test_dataset.map(lambda x, y: x['Query_tokens'])\n",
    "\n",
    "#Categorical Variable columns\n",
    "gender_test_ds = test_dataset.map(lambda x, y: x['Gender'])\n",
    "age_test_ds = test_dataset.map(lambda x, y: x['Age'])\n",
    "depth_test_ds = test_dataset.map(lambda x, y: x['Depth'])\n",
    "position_test_ds = test_dataset.map(lambda x, y: x['Position'])\n",
    "AdvertiserId_test_ds = test_dataset.map(lambda x, y: x['AdvertiserId'])\n",
    "AdId_test_ds = test_dataset.map(lambda x, y: x['AdId'])\n",
    "url_test_ds = test_dataset.map(lambda x, y: x['DisplayURL'])\n",
    "\n",
    "#Numerical Columns\n",
    "pospdepth_test_ds = test_dataset.map(lambda x, y: x['pos_per_depth'])\n",
    "\n",
    "#Label column\n",
    "y_test_ds = test_dataset.map(lambda x, y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = tf.data.Dataset.zip((adkeyword_test_ds, addescription_test_ds, adtitle_test_ds, query_test_ds, gender_test_ds, position_test_ds, age_test_ds, depth_test_ds, AdvertiserId_test_ds, AdId_test_ds, url_test_ds))\n",
    "dataset_test = tf.data.Dataset.zip((data_test, y_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.predict(dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_predicitons_1702', 'a') as file:\n",
    "    for item in score:\n",
    "        file.write(\"%s\\n\" % item[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
